{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "from stable_baselines3 import PPO, A2C, DQN\n",
    "from sb3_contrib import RecurrentPPO\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "\n",
    "from sympy import Matrix, symbols, eye, KroneckerProduct\n",
    "\n",
    "import warnings\n",
    "\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import pickle\n",
    "\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "\n",
    "# Suppress RuntimeWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available. Make sure to enable GPU in the runtime settings.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available.\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"GPU is not available. Make sure to enable GPU in the runtime settings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Configuration and Hyperparameters\n",
    "######################################\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "ALPHA = 0.7  # Leakage weight\n",
    "BETA = 0.1   # Closeness weight\n",
    "GAMMA = 0.2  # Unitarity weight\n",
    "SEQUENCE_LENGTH = 40  # Number of compositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfoCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super(InfoCallback, self).__init__(verbose)\n",
    "        self.leakage_log = []\n",
    "        self.closeness_log = []\n",
    "        self.unitarity_log = []\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        dones = self.locals['dones']  # list[bool], length = number_of_envs\n",
    "        infos = self.locals['infos']  # list[dict], length = number_of_envs\n",
    "\n",
    "        for env_idx, done in enumerate(dones):\n",
    "            if done:\n",
    "                info = self.locals['infos'][env_idx]\n",
    "                \n",
    "                \n",
    "                # Extract environment-specific metrics from info\n",
    "                leakage = info.get(\"leakage\", None)\n",
    "                closeness_error = info.get(\"closeness_error\", None)\n",
    "                unitarity_error = info.get(\"unitarity_error\", None)\n",
    "                final_reward = info.get(\"final_reward\", None)\n",
    "                current_step = info.get(\"current_step\", None)\n",
    "\n",
    "                # Append them to the logs if available\n",
    "                if leakage is not None:\n",
    "                    self.leakage_log.append(leakage)\n",
    "                if closeness_error is not None:\n",
    "                    self.closeness_log.append(closeness_error)\n",
    "                if unitarity_error is not None:\n",
    "                    self.unitarity_log.append(unitarity_error)\n",
    "                if final_reward is not None:\n",
    "                    self.episode_rewards.append(final_reward)\n",
    "                if current_step is not None:\n",
    "                    self.episode_lengths.append(current_step)\n",
    "\n",
    "        # Return True to indicate the training can go on\n",
    "        return True\n",
    "    \n",
    "    def save_callback_data(self, path):\n",
    "        data = {\n",
    "            \"leakage_log\": self.leakage_log,\n",
    "            \"closeness_log\": self.closeness_log,\n",
    "            \"unitarity_log\": self.unitarity_log,\n",
    "            \"episode_rewards\": self.episode_rewards,\n",
    "            \"episode_lengths\": self.episode_lengths,\n",
    "        }\n",
    "        with open(path, \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "\n",
    "    def load_callback_data(self, path):\n",
    "        with open(path, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        self.leakage_log = data[\"leakage_log\"]\n",
    "        self.closeness_log = data[\"closeness_log\"]\n",
    "        self.unitarity_log = data[\"unitarity_log\"]\n",
    "        self.episode_rewards = data[\"episode_rewards\"]\n",
    "        self.episode_lengths = data[\"episode_lengths\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct_sum(A, B):\n",
    "    # Helper function to convert input to a SymPy Matrix\n",
    "    def to_matrix(x):\n",
    "        if isinstance(x, Matrix):\n",
    "            return x\n",
    "        else:\n",
    "            # Assume x is a scalar, convert to 1x1 Matrix\n",
    "            return Matrix([[x]])\n",
    "\n",
    "    # Convert inputs to matrices\n",
    "    A_matrix = to_matrix(A)\n",
    "    B_matrix = to_matrix(B)\n",
    "\n",
    "    # Check if A_matrix is square\n",
    "    if A_matrix.rows != A_matrix.cols:\n",
    "        raise ValueError(f\"Matrix A is not square: {A_matrix.rows}x{A_matrix.cols}\")\n",
    "\n",
    "    # Check if B_matrix is square\n",
    "    if B_matrix.rows != B_matrix.cols:\n",
    "        raise ValueError(f\"Matrix B is not square: {B_matrix.rows}x{B_matrix.cols}\")\n",
    "\n",
    "    # Dimensions\n",
    "    N = A_matrix.rows\n",
    "    M = B_matrix.rows\n",
    "\n",
    "    # Create a zero matrix of size (N+M) x (N+M)\n",
    "    C = Matrix.zeros(N + M, N + M)\n",
    "\n",
    "    # Assign A_matrix to the upper-left block\n",
    "    C[:N, :N] = A_matrix\n",
    "\n",
    "    # Assign B_matrix to the lower-right block\n",
    "    C[N:N+M, N:N+M] = B_matrix\n",
    "\n",
    "    return C\n",
    "\n",
    "def tensor_product(A, B):\n",
    "    return KroneckerProduct(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R matrix\n",
    "R_matrix = np.array([\n",
    "    [ np.exp(-4j * np.pi / 5), 0                      ],\n",
    "    [ 0                      , np.exp(3j * np.pi / 5) ]\n",
    "])\n",
    "\n",
    "R_tt1 = symbols(\"R_tt1\")  # Top-left diagonal\n",
    "R_ttt = symbols(\"R_ttt\")  # Bottom-right diagonal\n",
    "\n",
    "sym_R = Matrix([\n",
    "    [R_tt1, 0],\n",
    "    [0, R_ttt]\n",
    "])\n",
    "\n",
    "# F matrix\n",
    "phi = (1 + np.sqrt(5)) / 2  # Golden ratio\n",
    "F_matrix = np.array([\n",
    "    [ 1/phi          , np.sqrt(1/phi) ],\n",
    "    [ np.sqrt(1/phi) , -1/phi         ]\n",
    "])\n",
    "\n",
    "F_11 =  symbols(\"F_11\")\n",
    "F_12 =  symbols(\"F_12\")\n",
    "F_21 =  symbols(\"F_21\")\n",
    "F_22 =  symbols(\"F_22\")\n",
    "\n",
    "sym_F = Matrix([\n",
    "    [F_11, F_12],\n",
    "    [F_21, F_22]\n",
    "])\n",
    "\n",
    "# Substitution dictionary\n",
    "subs = {\n",
    "    R_tt1: R_matrix[0, 0],\n",
    "    R_ttt: R_matrix[1, 1],\n",
    "    F_11: F_matrix[0, 0],\n",
    "    F_12: F_matrix[1, 0],\n",
    "    F_21: F_matrix[0, 1],\n",
    "    F_22: F_matrix[1, 1]\n",
    "}\n",
    "\n",
    "# Permutation matrix\n",
    "I_2 = eye(2)\n",
    "\n",
    "I_5 = eye(5)\n",
    "I_5.row_swap(0, 3)\n",
    "P14 = I_5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Braid representations\n",
    "\n",
    "rho_1 = direct_sum(R_ttt, tensor_product(sym_R, I_2).doit())\n",
    "\n",
    "rho_2 = direct_sum(R_ttt, tensor_product(sym_F * sym_R * sym_F, I_2).doit())\n",
    "\n",
    "rho_3 = P14 * direct_sum(R_ttt, direct_sum(sym_R, sym_F * sym_R * sym_F)) * P14\n",
    "\n",
    "rho_4 = direct_sum(R_ttt, tensor_product(I_2, sym_F * sym_R * sym_F).doit())\n",
    "\n",
    "rho_5 = direct_sum(R_ttt, tensor_product(I_2, sym_R).doit())\n",
    "\n",
    "# CNOT gate\n",
    "cnot_gate = Matrix([\n",
    "    [1, 0, 0, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 0, 0, 1],\n",
    "    [0, 0, 1, 0]\n",
    "])\n",
    "\n",
    "dcnot_gate = Matrix([\n",
    "    1, 0, 0, 0,\n",
    "    0, 0, 1, 0,\n",
    "    0, 0, 0, 1,\n",
    "    0, 1, 0, 0\n",
    "])\n",
    "\n",
    "swap_gate = Matrix([\n",
    "    1, 0, 0, 0,\n",
    "    0, 0, 1, 0,\n",
    "    0, 1, 0, 0,\n",
    "    0, 0, 0, 1    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GateApproxEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, braid_gates, target_gate, subs, max_length,\n",
    "                 alpha, beta, gamma, local_equivalence_class):\n",
    "        super(GateApproxEnv, self).__init__()\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.local_equivalence_class = local_equivalence_class\n",
    "\n",
    "        # Precompute numeric versions of braid_gates\n",
    "        self.braid_gates = []\n",
    "        for g in braid_gates:\n",
    "            g_evaluated = g.subs(subs).evalf()\n",
    "            if any(sym.is_symbol for sym in g_evaluated):\n",
    "                raise ValueError(\"Not all symbols substituted in a braid gate.\")\n",
    "            self.braid_gates.append(np.array(g_evaluated.tolist(), dtype=complex))\n",
    "\n",
    "        # Precompute numeric target gate\n",
    "        t_evaluated = target_gate.subs(subs).evalf()\n",
    "        if any(sym.is_symbol for sym in t_evaluated):\n",
    "            raise ValueError(\"Not all symbols substituted in target_gate.\")\n",
    "        self.target_gate = np.array(t_evaluated.tolist(), dtype=complex)\n",
    "\n",
    "        # Action space: One action per braid gate\n",
    "        self.action_space = spaces.Discrete(len(self.braid_gates))\n",
    "\n",
    "        # Observation: Flattened real+imag parts of the 5x5 gate = 50-dim vector\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(50,), dtype=np.float64)\n",
    "\n",
    "        self.reset_composition()\n",
    "\n",
    "    def reset_composition(self):\n",
    "        self.current_composition = np.eye(5, dtype=complex)\n",
    "        self.current_length = 0\n",
    "        self.gate_stack = []\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.reset_composition()\n",
    "        self.current_step = 0\n",
    "\n",
    "        # Randomize the episode length \n",
    "        self.random_episode_length = random.randint(20, self.max_length)\n",
    "\n",
    "        # Compute initial errors (for reward shaping)\n",
    "        leak_err, uni_err, close_err, _ = self.compute_reward()\n",
    "        self.prev_leakage = leak_err\n",
    "        self.prev_unitarity_error = uni_err\n",
    "        self.prev_closeness_error = close_err\n",
    "\n",
    "        # Store a \"prev total error\" to measure improvement\n",
    "        self.prev_total_error = (self.alpha * leak_err \n",
    "                                 + self.beta * close_err \n",
    "                                 + self.gamma * uni_err)\n",
    "\n",
    "        return self._get_obs(), {}\n",
    "    \n",
    "    def take_action(self, action):\n",
    "        if self.current_length < self.max_length:\n",
    "            # Numeric matrix multiplication only\n",
    "            self.current_composition = self.current_composition @ self.braid_gates[action]\n",
    "            self.gate_stack.append(f\"{action}\")\n",
    "            self.current_length += 1\n",
    "        else:\n",
    "            print(\"Warning: Maximum composition length reached. No action taken.\")\n",
    "\n",
    "    def compute_reward(self):\n",
    "        # Current composition is already numeric\n",
    "        M = self.current_composition\n",
    "        T = self.target_gate\n",
    "\n",
    "        # Leakage: abs(M[0,0])\n",
    "        leakage = np.abs(M[0,0])\n",
    "        # Avoid dividing by zero if leakage is extremely small:\n",
    "        if leakage < 1e-12:\n",
    "            leakage = 1e-12\n",
    "        #leakage_error = 1.0 / leakage\n",
    "\n",
    "        leakage_error = (1.0 - leakage)\n",
    "\n",
    "        # Extract 4x4 submatrix\n",
    "        M_4x4 = M[1:5, 1:5]\n",
    "\n",
    "        # Unitarity check\n",
    "        UdagU = M_4x4.conjugate().T @ M_4x4\n",
    "        unitarity_error = self.schatten_p_norm(UdagU - np.eye(4), 1)\n",
    "\n",
    "        # Closeness to target\n",
    "        if self.local_equivalence_class:\n",
    "            closeness_error = self.local_equivalence_distance(T, M_4x4)\n",
    "        else:\n",
    "            # Schatten 2-norm difference between normalized gates\n",
    "            A_norm = self.schatten_p_norm(M_4x4, 2)\n",
    "            T_norm = self.schatten_p_norm(T, 2)\n",
    "\n",
    "            if A_norm < 1e-12:  # safeguard\n",
    "                A_norm = 1e-12\n",
    "            if T_norm < 1e-12:\n",
    "                T_norm = 1e-12\n",
    "\n",
    "            A_normalized = M_4x4 / A_norm\n",
    "            T_normalized = T / T_norm\n",
    "            closeness_error = self.schatten_p_norm(A_normalized - T_normalized, 2)\n",
    "\n",
    "        # Reward (negative weighted sum)\n",
    "        reward = - (self.alpha * leakage_error + self.beta * closeness_error + self.gamma * unitarity_error)\n",
    "        \n",
    "        return float(leakage), float(closeness_error), float(unitarity_error), float(reward)\n",
    "    \n",
    "    def step(self, action):\n",
    "\n",
    "        old_total_error = self.prev_total_error\n",
    "\n",
    "        # 2. Take action\n",
    "        self.take_action(action)\n",
    "        self.current_step += 1\n",
    "\n",
    "        # 3. Compute new error\n",
    "        leakage, closeness_error, unitarity_error, combined_reward = self.compute_reward()\n",
    "        new_total_error = (self.alpha * (1.0 - leakage) + self.beta * closeness_error + self.gamma * unitarity_error)\n",
    "\n",
    "        # 4. Provide incremental reward for improvement\n",
    "        # If we reduced total_error, that's an improvement => positive reward\n",
    "        improvement = old_total_error - new_total_error\n",
    "        step_reward = improvement \n",
    "\n",
    "        # Update stored \"previous\" values for next step\n",
    "        self.prev_leakage = leakage\n",
    "        self.prev_unitarity_error = unitarity_error\n",
    "        self.prev_closeness_error = closeness_error\n",
    "        self.prev_total_error = new_total_error\n",
    "\n",
    "        terminated = (self.current_step >= self.random_episode_length)\n",
    "        truncated = False\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        # If the episode terminates, add a final reward that reflects\n",
    "        # the final composition's closeness to target\n",
    "        if terminated:\n",
    "            step_reward += combined_reward\n",
    "\n",
    "            # Compute final 4x4 submatrix and Makhlin invariants:\n",
    "            M_4x4 = self.current_composition[1:5, 1:5]\n",
    "            g_1, g_2, g_3 = self.compute_makhlin_invariants(M_4x4)\n",
    "\n",
    "            info = {\n",
    "                \"current_step\": self.current_step,\n",
    "                \"gate_stack\": self.gate_stack.copy(),\n",
    "                \"leakage\": leakage,\n",
    "                \"closeness_error\": closeness_error,\n",
    "                \"unitarity_error\": unitarity_error,\n",
    "                \"combined_reward\": combined_reward,\n",
    "                \"improvement\": improvement,\n",
    "                \"final_reward\": step_reward if terminated else None,\n",
    "                \"g_1\":g_1,\n",
    "                \"g_2\":g_2,\n",
    "                \"g_3\":g_3,\n",
    "                \"M_4x4\": M_4x4\n",
    "            }\n",
    "\n",
    "        obs = self._get_obs()\n",
    "\n",
    "        return obs, step_reward, terminated, truncated, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        # current_composition is numeric, just flatten\n",
    "        M = self.current_composition\n",
    "        obs = np.concatenate([M.real.flatten(), M.imag.flatten()])\n",
    "        return obs\n",
    "\n",
    "    \"\"\"\n",
    "    def schatten_p_norm(self, T, p):\n",
    "\n",
    "        # Compute |T| = sqrt(T^† T)\n",
    "        abs_T = sqrtm(T.conj().T @ T)\n",
    "\n",
    "        # Compute |T|^p\n",
    "        abs_T_p = np.linalg.matrix_power(abs_T, p)\n",
    "\n",
    "        # Compute the trace of |T|^p\n",
    "        trace_value = np.trace(abs_T_p)\n",
    "\n",
    "        # Compute the Schatten p-norm\n",
    "        schatten_norm = np.real(trace_value)**(1/p)\n",
    "\n",
    "        return schatten_norm\n",
    "    \"\"\"\n",
    "\n",
    "    def schatten_p_norm(self, T, p):\n",
    "        # Ensure T is a NumPy array\n",
    "        T = np.array(T, dtype=complex)\n",
    "\n",
    "        # Compute singular values of T\n",
    "        singular_values = np.linalg.svd(T, compute_uv=False)\n",
    "\n",
    "        # Compute Schatten p-norm\n",
    "        if p == np.inf:  # Special case for p = infinity\n",
    "            schatten_norm = np.max(singular_values)\n",
    "        elif p == 1:  # Special case for p = 1 (nuclear norm)\n",
    "            schatten_norm = np.sum(singular_values)\n",
    "        else:\n",
    "            # General case for arbitrary p\n",
    "            schatten_norm = (np.sum(singular_values**p))**(1/p)\n",
    "\n",
    "        return schatten_norm\n",
    "\n",
    "    def compute_makhlin_invariants(self, U):\n",
    "\n",
    "        i = 1j  # Complex unit (sqrt(-1))\n",
    "        Q = (1 / np.sqrt(2)) * np.array([\n",
    "            [1,  0,  0,  i],\n",
    "            [0,  i,  1,  0],\n",
    "            [0,  i, -1,  0],\n",
    "            [1,  0,  0, -i]\n",
    "        ], dtype=complex)\n",
    "\n",
    "        # Compute U_B = Q^\\dagger U Q\n",
    "        U_B = Q.conjugate().T @ U @ Q\n",
    "\n",
    "        # Makhlin matrix: m_U = (U_B)^T U_B\n",
    "        m_U = (U_B.T) @ U_B\n",
    "\n",
    "        # Compute trace and related quantities\n",
    "        tr_mU = np.trace(m_U)\n",
    "        tr_mU2 = np.trace(m_U @ m_U)\n",
    "        det_U = np.linalg.det(U)  # determinant of U\n",
    "\n",
    "        if np.abs(det_U) < 1e-12:\n",
    "            print(\"Warning: Determinant is very small, adding regularization.\")\n",
    "            det_U += 1e-12\n",
    "\n",
    "\n",
    "        # Compute complex quantity: (tr^2(m_U) / (16 * det(U)))\n",
    "        complex_val = (tr_mU**2) / (16.0 * det_U)\n",
    "\n",
    "        # g_1 = Re{complex_val}\n",
    "        g_1 = complex_val.real\n",
    "\n",
    "        # g_2 = Im{complex_val}\n",
    "        g_2 = complex_val.imag\n",
    "\n",
    "        # g_3 = (tr^2(m_U) - tr(m_U^2)) / (4 * det(U))\n",
    "        g_3 = ((tr_mU**2) - tr_mU2) / (4.0 * det_U)\n",
    "\n",
    "        return (g_1, g_2, g_3)\n",
    "\n",
    "    def local_equivalence_distance(self, E, U):\n",
    "        # Compute Makhlin invariants for E and U\n",
    "        gE = self.compute_makhlin_invariants(E)\n",
    "        gU = self.compute_makhlin_invariants(U)\n",
    "\n",
    "        # Compute Δg_i and sum their squares\n",
    "        diff_squares = [(abs(e - u))**2 for e, u in zip(gE, gU)]\n",
    "        d_EU = sum(diff_squares)\n",
    "        return d_EU\n",
    "    \n",
    "\n",
    "\n",
    "    def get_gate_composition(self, gate_string: str) -> np.ndarray:\n",
    "        composition = np.eye(5, dtype=complex)\n",
    "        for char in gate_string:\n",
    "            try:\n",
    "                gate_index = int(char)\n",
    "            except ValueError:\n",
    "                raise ValueError(f\"Invalid character '{char}' in gate string. Must be a digit.\")\n",
    "\n",
    "            if gate_index < 0 or gate_index >= len(self.braid_gates):\n",
    "                raise IndexError(f\"Gate index {gate_index} is out of bounds for the available braid gates.\")\n",
    "\n",
    "            composition = composition @ self.braid_gates[gate_index]\n",
    "        return composition\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#braid_gates = [rho_1, rho_2, rho_3, rho_4, rho_5]\n",
    "\n",
    "\n",
    "braid_gates = [\n",
    "        rho_1,       rho_2,       rho_3,       rho_4,       rho_5,\n",
    "        rho_1.inv(), rho_2.inv(), rho_3.inv(), rho_4.inv(), rho_5.inv()\n",
    "    ]\n",
    "\n",
    "\n",
    "target_gate = cnot_gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gate_approx_env(env_kwargs):\n",
    "    def _init():\n",
    "        env = GateApproxEnv(**env_kwargs)\n",
    "  \n",
    "        # env.seed(seed + rank)  \n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "env_kwargs = {\n",
    "    \"braid_gates\": braid_gates,        \n",
    "    \"target_gate\": target_gate,        \n",
    "    \"subs\": subs,                      \n",
    "    \"max_length\": SEQUENCE_LENGTH,                  \n",
    "    \"alpha\": ALPHA,\n",
    "    \"beta\": BETA,\n",
    "    \"gamma\": GAMMA,\n",
    "    \"local_equivalence_class\": True\n",
    "}\n",
    "\n",
    "num_envs = 8  # number of parallel processes\n",
    "\n",
    "# Build a list of environment-initializer callables\n",
    "env_fns = [make_gate_approx_env(env_kwargs)\n",
    "           for i in range(num_envs)]\n",
    "\n",
    "# Create the SubprocVecEnv\n",
    "vec_env = SubprocVecEnv(env_fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_timesteps = 30000\n",
    "verbose = 1\n",
    "\n",
    "policy_kwargs = dict(net_arch=[512, 256, 128, 64])\n",
    "model = RecurrentPPO(\n",
    "    \"MlpLstmPolicy\",\n",
    "    vec_env,\n",
    "    #env,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    #device=\"cuda\",\n",
    "    verbose=verbose,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    gamma=0.9,\n",
    "    tensorboard_log=\"./tensorboard_logs/\"\n",
    "    )\n",
    "\n",
    "info_callback = InfoCallback()\n",
    "model.learn(total_timesteps=total_timesteps, callback=info_callback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_metrics(callback: InfoCallback):\n",
    "    # We have as many episodes as the length of the final_reward log,\n",
    "    # or closeness_log, or leakage_log -- ideally they should match\n",
    "    episodes = range(1, len(callback.episode_rewards) + 1)\n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n",
    "\n",
    "    \n",
    "\n",
    "    # Episode Final Reward\n",
    "    axs[0, 0].plot(episodes, callback.episode_rewards, label='Final Episode Reward')\n",
    "    #axs[0, 0].set_yscale('log')\n",
    "    axs[0, 0].set_xlabel('Episode')\n",
    "    axs[0, 0].set_ylabel('Reward')\n",
    "    axs[0, 0].set_title('Episode Final Rewards')\n",
    "    axs[0, 0].legend()\n",
    "\n",
    "    # Leakage\n",
    "    axs[0, 1].plot(episodes, callback.leakage_log, color='orange', label='Leakage')\n",
    "    axs[0, 1].set_xlabel('Episode')\n",
    "    axs[0, 1].set_ylabel('Leakage')\n",
    "    axs[0, 1].set_title('Leakage per Episode')\n",
    "    axs[0, 1].legend()\n",
    "\n",
    "    # Closeness Error\n",
    "    axs[1, 0].plot(episodes, callback.closeness_log, color='green', label='Closeness Error')\n",
    "    #axs[1, 0].set_yscale('log')\n",
    "    axs[1, 0].set_xlabel('Episode')\n",
    "    axs[1, 0].set_ylabel('Closeness Error')\n",
    "    axs[1, 0].set_title('Closeness per Episode')\n",
    "    axs[1, 0].legend()\n",
    "\n",
    "    # Unitarity Error\n",
    "    axs[1, 1].plot(episodes, callback.unitarity_log, color='red', label='Unitarity Error')\n",
    "    axs[1, 1].set_xlabel('Episode')\n",
    "    axs[1, 1].set_ylabel('Unitarity Error')\n",
    "    axs[1, 1].set_title('Unitarity per Episode')\n",
    "    axs[1, 1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the main metrics\n",
    "plot_training_metrics(info_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_low_unitary_acceleration(callback, threshold=0.001, increment=1.0):\n",
    "    # Convert the unitarity log into a NumPy array.\n",
    "    errors = np.array(callback.unitarity_log)\n",
    "    num_episodes = len(errors)\n",
    "    \n",
    "    # Initialize the cumulative array.\n",
    "    cumulative = np.zeros(num_episodes)\n",
    "    \n",
    "    # For each episode, if the unitarity error is below threshold,\n",
    "    # subtract 'decrement' from the cumulative value.\n",
    "    for i in range(num_episodes):\n",
    "        if i > 0:\n",
    "            cumulative[i] = cumulative[i-1]\n",
    "        if errors[i] < threshold:\n",
    "            cumulative[i] += increment\n",
    "\n",
    "    episodes = np.arange(1, num_episodes + 1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(episodes, cumulative, marker='.', linestyle='-')\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Cumulative Low-Error Score\")\n",
    "    plt.title(\"Acceleration of Achieving Low Unitarity Error\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_low_leakage_acceleration(callback, threshold=0.9, increment=1.0):\n",
    "    # Get leakage errors as a NumPy array.\n",
    "    leakage_errors = np.array(callback.leakage_log)\n",
    "    num_episodes = len(leakage_errors)\n",
    "    \n",
    "    # Initialize the cumulative score array.\n",
    "    cumulative = np.zeros(num_episodes)\n",
    "    \n",
    "    # For each episode, if leakage is below the threshold, decrement the cumulative score.\n",
    "    for i in range(num_episodes):\n",
    "        if i > 0:\n",
    "            cumulative[i] = cumulative[i-1]\n",
    "        if leakage_errors[i] > threshold:\n",
    "            cumulative[i] += increment\n",
    "\n",
    "    episodes = np.arange(1, num_episodes + 1)\n",
    "    \n",
    "    # Plot the cumulative score.\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(episodes, cumulative, marker='.', linestyle='-')\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Cumulative Low-Leakage Score\")\n",
    "    plt.title(\"Acceleration of Achieving Low Leakage\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "plot_low_unitary_acceleration(info_callback, threshold=0.0000000000001, increment=1.0)\n",
    "\n",
    "# Example usage:\n",
    "plot_low_leakage_acceleration(info_callback, threshold=0.95, increment=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy_vec(env, model, num_sequences, csv_filename=\"results_lec.csv\"):\n",
    "    SEQUENCE_LENGTH = getattr(env, 'max_length', 10)\n",
    "    max_operator_width = SEQUENCE_LENGTH\n",
    "    operator_column_width = max(max_operator_width, 10)\n",
    "\n",
    "    # Open CSV file for writing\n",
    "    with open(csv_filename, \"w\", newline=\"\") as csvfile:\n",
    "        fieldnames = [\"Closeness\", \"Leakage\", \"Unitarity\", \"Operator\"]\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        # Reset all parallel environments at once\n",
    "        obs = env.reset()  # Assuming single return value\n",
    "        episodes_collected = 0\n",
    "\n",
    "        while episodes_collected < num_sequences:\n",
    "            actions, _ = model.predict(obs, deterministic=False)\n",
    "            obs, _, dones, infos = env.step(actions)\n",
    "\n",
    "            for i in range(env.num_envs):\n",
    "                if dones[i]:\n",
    "                    episodes_collected += 1\n",
    "                    gate_stack = infos[i].get(\"gate_stack\", [])\n",
    "                    operator_string = ''.join(map(str, gate_stack))\n",
    "\n",
    "                    closeness_error = infos[i].get(\"closeness_error\", 9999)\n",
    "                    leakage = infos[i].get(\"leakage\", 0.0)\n",
    "                    unitarity_error = infos[i].get(\"unitarity_error\", 9999)\n",
    "\n",
    "                    # Retrieve Makhlin invariants if needed:\n",
    "                    # g_1 = infos[i].get(\"g_1\", float('nan'))\n",
    "                    # g_2 = infos[i].get(\"g_2\", float('nan'))\n",
    "                    # g_3 = infos[i].get(\"g_3\", float('nan'))\n",
    "\n",
    "                    # Output row only if the condition is met:\n",
    "                    if closeness_error < .0001 and (leakage > 0.98):\n",
    "                    #if (closeness_error < 0.9) and (leakage > 0.8) and (unitarity_error < 1e-10):\n",
    "                        writer.writerow({\n",
    "                            \"Closeness\": f\"{closeness_error:.4e}\",\n",
    "                            \"Leakage\": f\"{leakage:.3f}\",\n",
    "                            \"Unitarity\": f\"{unitarity_error:.3e}\",\n",
    "                            \"Operator\": operator_string\n",
    "                        })\n",
    "\n",
    "                    if episodes_collected >= num_sequences:\n",
    "                        break\n",
    "\n",
    "            if episodes_collected >= num_sequences:\n",
    "                break\n",
    "\n",
    "    print(f\"Results exported to {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = PPO.load(\"working_model\", env=vec_env) # 21 to 40\n",
    "\n",
    "#loaded_model = PPO.load(\"length_3_inv_model\", env=vec_env)\n",
    "#loaded_model = PPO.load(\"length_4_inv_model\", env=vec_env)\n",
    "#loaded_model = PPO.load(\"length_5_inv_model\", env=vec_env)\n",
    "#loaded_model = PPO.load(\"length_6_inv_model\", env=vec_env)\n",
    "#loaded_model = PPO.load(\"length_7_inv_model\", env=vec_env)\n",
    "\n",
    "#loaded_model = PPO.load(\"length_10_inv_model\", env=vec_env)\n",
    "#loaded_model = PPO.load(\"length_20_inv_model\", env=vec_env)\n",
    "#loaded_model = PPO.load(\"length_21_inv_model\", env=vec_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_policy_vec(vec_env, loaded_model, num_sequences=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save(\"length_20_inv_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The following is test code and is inherently unstructured and messy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "gates = [\n",
    "    \"7397799322\", # 10\n",
    "    \"373373739737937373373\", #21\n",
    "    \"373373733743000773323\", #21\n",
    "    \"373323393737332975373\" #21\n",
    "    \"373373787477337373373\", #21\n",
    "    \"373373733743035223733\", #21\n",
    "    \"373373747337334323737\", #21\n",
    "    \"70747074804807470770\", #20\n",
    "    \"74715417175771747519\" #20\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_gate_composition(gate_string: str, braid_gates: list) -> np.ndarray:\n",
    "    # Initialize composition as the 5x5 identity matrix.\n",
    "    composition = np.eye(5, dtype=complex)\n",
    "\n",
    "    # Iterate over each character in the string.\n",
    "    for char in gate_string:\n",
    "        # Convert the character to an integer index.\n",
    "        try:\n",
    "            gate_index = int(char)\n",
    "        except ValueError:\n",
    "            raise ValueError(f\"Invalid character '{char}' in gate string. Must be a digit.\")\n",
    "\n",
    "        # Check that the index is within the valid range.\n",
    "        if gate_index < 0 or gate_index >= len(braid_gates):\n",
    "            raise IndexError(f\"Gate index {gate_index} is out of bounds for the available braid gates.\")\n",
    "\n",
    "        # Multiply the current composition by the braid gate.\n",
    "        composition = composition @ braid_gates[gate_index]\n",
    "    \n",
    "    return composition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gate_metrics(gate_strings, env, csv_filename=\"gate_metrics.csv\"):\n",
    "    # Determine the maximum width for the \"Operator\" column for nice formatting.\n",
    "    max_operator_width = max(len(str(g)) for g in gate_strings)\n",
    "    operator_column_width = max(max_operator_width, 10)  # Ensure at least 10 characters for aesthetics\n",
    "\n",
    "    # Print table header to the console\n",
    "    header = (f\"{'Operator':<{operator_column_width}} | \"\n",
    "              f\"{'Closeness':<10} | {'Leakage':<10} | {'Unitarity':<10}\")\n",
    "    print(header)\n",
    "    print(\"-\" * (operator_column_width + 34))\n",
    "\n",
    "    # Open CSV file for writing the results.\n",
    "    with open(csv_filename, \"w\", newline=\"\") as csvfile:\n",
    "        fieldnames = [\"Operator\", \"Closeness\", \"Leakage\", \"Unitarity\"]\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        # Process each gate string.\n",
    "        for gate_str in gate_strings:\n",
    "            # Reset the current composition in the environment.\n",
    "            env.reset_composition()\n",
    "            \n",
    "            # Compute the gate composition for the given string.\n",
    "            gate_matrix = env.get_gate_composition(gate_str)\n",
    "            \n",
    "            # Update the environment's composition to the computed gate.\n",
    "            env.current_composition = gate_matrix\n",
    "            \n",
    "            # Compute the metrics for the current composition.\n",
    "            leakage, closeness_error, unitarity_error, total_error = env.compute_reward()\n",
    "            \n",
    "            # Optionally, set unitarity error to exactly zero if very small.\n",
    "            if unitarity_error < 0.0001:\n",
    "                unitarity_error = 0.0\n",
    "            \n",
    "            # Format and print the row with the gate metrics.\n",
    "            print(f\"{gate_str:<{operator_column_width}} | \"\n",
    "                  f\"{closeness_error:<10.3e} | \"\n",
    "                  f\"{leakage:<10.3f} | \"\n",
    "                  f\"{unitarity_error:<10.3e}\")\n",
    "            \n",
    "            # Write the row to the CSV file\n",
    "            writer.writerow({\n",
    "                \"Operator\": gate_str,\n",
    "                \"Closeness\": f\"{closeness_error:.3e}\",\n",
    "                \"Leakage\": f\"{leakage:.3f}\",\n",
    "                \"Unitarity\": f\"{unitarity_error:.3e}\"\n",
    "            })\n",
    "\n",
    "    print(f\"Gate metrics have been written to {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "test = [\"30\",\n",
    "        \"880\",\n",
    "        \"5300\",\n",
    "        \"48440\",\n",
    "        \"553000\",\n",
    "        \"4334300\",\n",
    "        \"37770000\",\n",
    "        \"441001048\",\n",
    "        \"2555337582\",\n",
    "        \"59672955693\",\n",
    "        \"374050007970\",\n",
    "        \"306947382105\",\n",
    "        \"6595888969003\",\n",
    "        \"75139699757375\",\n",
    "        \"78450807782427077787\",\n",
    "        \"287430798424700981936977\"\n",
    "        ]\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "test = [\n",
    "    \"000\",\n",
    "    \"0000\",\n",
    "    \"00000\",\n",
    "    \"222000\",\n",
    "    \"0000000\",\n",
    "    \"00000000\",\n",
    "    \"000000000\",\n",
    "    \"2221001222\",\n",
    "    \"22210012220\",\n",
    "    \"223443100122\",\n",
    "    \"2213404301242\",\n",
    "    \"34224334310031224\",\n",
    "    \"242314040312211412221\",\n",
    "    \"422211222020214000112021\",\n",
    "    \"101422102223130431111322222\",\n",
    "    \"31423322434332420442310301422\",\n",
    "    \"222223043422422024043333320003\",\n",
    "    \"2344130440331032213334400344312\",\n",
    "    \"423330314001220244224333334034032\",\n",
    "    \"132424244140142042111112011202122020400\",\n",
    "    \"231333001244422220433403402422343302043\"\n",
    "]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_gates = [\n",
    "    \"373373739737937373373\", #21\n",
    "    \"373373733743000773323\", #21\n",
    "    \"373323393737332975373\" #21\n",
    "    \"373373787477337373373\", #21\n",
    "    \"373373733743035223733\", #21\n",
    "    \"373373747337334323737\", #21\n",
    "    \"70747074804807470770\", #20\n",
    "    \"74715417175771747519\", #20\n",
    "    \"124543537528586425295552041721248\", \n",
    "    \"3304508631218099892215222273419788357195\",\n",
    "    \"2522122520252241121222952\",\n",
    "    \"2122521290220262257522525\",\n",
    "    \"212212922221206520262657\",\n",
    "    \"252282524225252528922929527272252\",\n",
    "    \"29220239322922922922920222292927209229\",\n",
    "    \"7545960196017659606465541914747\",\n",
    "    \"226262460644262626167206626272090226\",\n",
    "    \"246060662626072722621262727272727206626\",\n",
    "    \"272776262720226242026442267\",\n",
    "    \"27272722626202724625625226244616\",\n",
    "    \"27226262721242606620229262026226262622\",\n",
    "    \"7397799322\",\n",
    "    \"373373739737937373373\",\n",
    "    \"373373733743000773323\",\n",
    "    \"2262620207742620262\",\n",
    "    \"206724676460242606626764692627272720202\",\n",
    "    \"226262020662676402622629720127202662\",\n",
    "    \"2262627262020662620962262\",\n",
    "    \"22626246064426262606626242026\",\n",
    "    \"2262624672064427262672722672066262427242\",\n",
    "    \"272262627246727292020662620962267922\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_sequence(seq: str) -> str:\n",
    "    \n",
    "    # Inverse mapping: each key is the inverse of its value.\n",
    "    inv_map = {\n",
    "        '0': '5', '1': '6', '2': '7', '3': '8', '4': '9',\n",
    "        '5': '0', '6': '1', '7': '2', '8': '3', '9': '4'\n",
    "    }\n",
    "    \n",
    "    stack = []\n",
    "    for char in seq:\n",
    "        # Check if the last element in stack is the inverse of current char.\n",
    "        if stack and inv_map[char] == stack[-1]:\n",
    "            stack.pop()\n",
    "        else:\n",
    "            stack.append(char)\n",
    "    \n",
    "    return ''.join(stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_found_gates = []\n",
    "\n",
    " # Process and print the reduced sequences\n",
    "for i, seq in enumerate(found_gates, 1):\n",
    "    reduced = reduce_sequence(seq)\n",
    "    print(f\"Sequence {i}: Original: {seq}\")\n",
    "    print(f\"            Reduced:  {reduced}\\n\")\n",
    "    reduced_found_gates.append(reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GateApproxEnv(\n",
    "    braid_gates=braid_gates,\n",
    "    target_gate=target_gate,\n",
    "    subs=subs,\n",
    "    max_length=SEQUENCE_LENGTH,\n",
    "    alpha=ALPHA,\n",
    "    beta=BETA,\n",
    "    gamma=GAMMA,\n",
    "    local_equivalence_class=True\n",
    ")\n",
    "print_gate_metrics(reduced_found_gates, env)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
