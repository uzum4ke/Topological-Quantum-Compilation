{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "from stable_baselines3 import PPO, A2C, DQN\n",
    "from sb3_contrib import RecurrentPPO\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "\n",
    "from sympy import Matrix, symbols, eye, KroneckerProduct\n",
    "\n",
    "import warnings\n",
    "\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import pickle\n",
    "\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "\n",
    "# Suppress RuntimeWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, torch, random\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available. Make sure to enable GPU in the runtime settings.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available.\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"GPU is not available. Make sure to enable GPU in the runtime settings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Configuration and Hyperparameters\n",
    "######################################\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "ALPHA = 0.7  # Leakage weight\n",
    "BETA = 0.1   # Closeness weight\n",
    "GAMMA = 0.2  # Unitarity weight\n",
    "SEQUENCE_LENGTH = 40  # Number of compositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfoCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for logging environment metrics at the end of each episode.\n",
    "\n",
    "    Logs leakage, closeness error, unitarity error, total episode reward,\n",
    "    and episode length. Provides methods to save and load log data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, verbose=0):\n",
    "        \"\"\"\n",
    "        Initialize the InfoCallback.\n",
    "\n",
    "        :param verbose: Verbosity level (0 = no output, >0 = printed messages).\n",
    "        \"\"\"\n",
    "\n",
    "        super(InfoCallback, self).__init__(verbose)\n",
    "        self.leakage_log = []\n",
    "        self.closeness_log = []\n",
    "        self.unitarity_log = []\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        \"\"\"\n",
    "        Called at the end of each rollout collection.\n",
    "        Can be used to process or visualize rollout-level data.\n",
    "        Currently not used.\n",
    "        \"\"\"\n",
    "\n",
    "        pass\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"\n",
    "        Called at every environment step. Checks for episode completion\n",
    "        (done flag) and logs final metrics from 'infos'.\n",
    "\n",
    "        :return: True to continue training, False to stop.\n",
    "        \"\"\"\n",
    "        dones = self.locals['dones']  # list[bool], length = number_of_envs\n",
    "        infos = self.locals['infos']  # list[dict], length = number_of_envs\n",
    "\n",
    "        for env_idx, done in enumerate(dones):\n",
    "            if done:\n",
    "                info = self.locals['infos'][env_idx]\n",
    "                \n",
    "                \n",
    "                # Extract environment-specific metrics from info\n",
    "                leakage = info.get(\"leakage\", None)\n",
    "                closeness_error = info.get(\"closeness_error\", None)\n",
    "                unitarity_error = info.get(\"unitarity_error\", None)\n",
    "                final_reward = info.get(\"final_reward\", None)\n",
    "                current_step = info.get(\"current_step\", None)\n",
    "\n",
    "                # Append them to the logs if available\n",
    "                if leakage is not None:\n",
    "                    self.leakage_log.append(leakage)\n",
    "                if closeness_error is not None:\n",
    "                    self.closeness_log.append(closeness_error)\n",
    "                if unitarity_error is not None:\n",
    "                    self.unitarity_log.append(unitarity_error)\n",
    "                if final_reward is not None:\n",
    "                    self.episode_rewards.append(final_reward)\n",
    "                if current_step is not None:\n",
    "                    self.episode_lengths.append(current_step)\n",
    "\n",
    "        # Return True to indicate the training can go on\n",
    "        return True\n",
    "    \n",
    "    def save_callback_data(self, path):\n",
    "        \"\"\"\n",
    "        Save the logged data to a file via pickle.\n",
    "\n",
    "        :param path: Filepath to write the pickled log data.\n",
    "        \"\"\"\n",
    "        data = {\n",
    "            \"leakage_log\": self.leakage_log,\n",
    "            \"closeness_log\": self.closeness_log,\n",
    "            \"unitarity_log\": self.unitarity_log,\n",
    "            \"episode_rewards\": self.episode_rewards,\n",
    "            \"episode_lengths\": self.episode_lengths,\n",
    "        }\n",
    "        with open(path, \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "\n",
    "    def load_callback_data(self, path):\n",
    "        \"\"\"\n",
    "        Load previously saved log data from a pickle file.\n",
    "\n",
    "        :param path: Filepath from which to load the log data.\n",
    "        \"\"\"\n",
    "        with open(path, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        self.leakage_log = data[\"leakage_log\"]\n",
    "        self.closeness_log = data[\"closeness_log\"]\n",
    "        self.unitarity_log = data[\"unitarity_log\"]\n",
    "        self.episode_rewards = data[\"episode_rewards\"]\n",
    "        self.episode_lengths = data[\"episode_lengths\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct_sum(A, B):\n",
    "    \"\"\"\n",
    "    Compute the direct (block-diagonal) sum of two square matrices or scalars.\n",
    "\n",
    "    If A or B is a scalar, it is treated as a 1×1 Matrix.\n",
    "\n",
    "    The result is a (n+m)×(n+m) Matrix:\n",
    "        [ A   0 ]\n",
    "        [ 0   B ]\n",
    "\n",
    "    Args:\n",
    "        A: A square SymPy Matrix or scalar.\n",
    "        B: A square SymPy Matrix or scalar.\n",
    "\n",
    "    Returns:\n",
    "        A new SymPy Matrix of size (n+m)×(n+m), with A in the top-left\n",
    "        block and B in the bottom-right block.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: if A or B (when converted to Matrix) is not square.\n",
    "    \"\"\"\n",
    "\n",
    "    # Helper function to convert input to a SymPy Matrix\n",
    "    def to_matrix(x):\n",
    "        if isinstance(x, Matrix):\n",
    "            return x\n",
    "        else:\n",
    "            # Assume x is a scalar, convert to 1x1 Matrix\n",
    "            return Matrix([[x]])\n",
    "\n",
    "    # Convert inputs to matrices\n",
    "    A_matrix = to_matrix(A)\n",
    "    B_matrix = to_matrix(B)\n",
    "\n",
    "    # Check if A_matrix is square\n",
    "    if A_matrix.rows != A_matrix.cols:\n",
    "        raise ValueError(f\"Matrix A is not square: {A_matrix.rows}x{A_matrix.cols}\")\n",
    "\n",
    "    # Check if B_matrix is square\n",
    "    if B_matrix.rows != B_matrix.cols:\n",
    "        raise ValueError(f\"Matrix B is not square: {B_matrix.rows}x{B_matrix.cols}\")\n",
    "\n",
    "    # Dimensions\n",
    "    N = A_matrix.rows\n",
    "    M = B_matrix.rows\n",
    "\n",
    "    # Create a zero matrix of size (N+M) x (N+M)\n",
    "    C = Matrix.zeros(N + M, N + M)\n",
    "\n",
    "    # Assign A_matrix to the upper-left block\n",
    "    C[:N, :N] = A_matrix\n",
    "\n",
    "    # Assign B_matrix to the lower-right block\n",
    "    C[N:N+M, N:N+M] = B_matrix\n",
    "\n",
    "    return C\n",
    "\n",
    "def tensor_product(A, B):\n",
    "    \"\"\"\n",
    "    Compute the Kronecker (tensor) product of two matrices.\n",
    "\n",
    "    Args:\n",
    "        A: SymPy Matrix.\n",
    "        B: SymPy Matrix.\n",
    "\n",
    "    Returns:\n",
    "        The Kronecker product A ⊗ B as a SymPy Matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    return KroneckerProduct(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R matrix\n",
    "R_matrix = np.array([\n",
    "    [ np.exp(-4j * np.pi / 5), 0                      ],\n",
    "    [ 0                      , np.exp(3j * np.pi / 5) ]\n",
    "])\n",
    "\n",
    "R_tt1 = symbols(\"R_tt1\")  # Top-left diagonal\n",
    "R_ttt = symbols(\"R_ttt\")  # Bottom-right diagonal\n",
    "\n",
    "sym_R = Matrix([\n",
    "    [R_tt1, 0],\n",
    "    [0, R_ttt]\n",
    "])\n",
    "\n",
    "# F matrix\n",
    "phi = (1 + np.sqrt(5)) / 2  # Golden ratio\n",
    "F_matrix = np.array([\n",
    "    [ 1/phi          , np.sqrt(1/phi) ],\n",
    "    [ np.sqrt(1/phi) , -1/phi         ]\n",
    "])\n",
    "\n",
    "F_11 =  symbols(\"F_11\")\n",
    "F_12 =  symbols(\"F_12\")\n",
    "F_21 =  symbols(\"F_21\")\n",
    "F_22 =  symbols(\"F_22\")\n",
    "\n",
    "sym_F = Matrix([\n",
    "    [F_11, F_12],\n",
    "    [F_21, F_22]\n",
    "])\n",
    "\n",
    "# Substitution dictionary\n",
    "subs = {\n",
    "    R_tt1: R_matrix[0, 0],\n",
    "    R_ttt: R_matrix[1, 1],\n",
    "    F_11: F_matrix[0, 0],\n",
    "    F_12: F_matrix[1, 0],\n",
    "    F_21: F_matrix[0, 1],\n",
    "    F_22: F_matrix[1, 1]\n",
    "}\n",
    "\n",
    "# Permutation matrix\n",
    "I_2 = eye(2)\n",
    "\n",
    "I_5 = eye(5)\n",
    "I_5.row_swap(0, 3)\n",
    "P14 = I_5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Braid representations\n",
    "\n",
    "rho_1 = direct_sum(R_ttt, tensor_product(sym_R, I_2).doit())\n",
    "\n",
    "rho_2 = direct_sum(R_ttt, tensor_product(sym_F * sym_R * sym_F, I_2).doit())\n",
    "\n",
    "rho_3 = P14 * direct_sum(R_ttt, direct_sum(sym_R, sym_F * sym_R * sym_F)) * P14\n",
    "\n",
    "rho_4 = direct_sum(R_ttt, tensor_product(I_2, sym_F * sym_R * sym_F).doit())\n",
    "\n",
    "rho_5 = direct_sum(R_ttt, tensor_product(I_2, sym_R).doit())\n",
    "\n",
    "# CNOT gate\n",
    "cnot_gate = Matrix([\n",
    "    [1, 0, 0, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 0, 0, 1],\n",
    "    [0, 0, 1, 0]\n",
    "])\n",
    "\n",
    "dcnot_gate = Matrix([\n",
    "    1, 0, 0, 0,\n",
    "    0, 0, 1, 0,\n",
    "    0, 0, 0, 1,\n",
    "    0, 1, 0, 0\n",
    "])\n",
    "\n",
    "swap_gate = Matrix([\n",
    "    1, 0, 0, 0,\n",
    "    0, 0, 1, 0,\n",
    "    0, 1, 0, 0,\n",
    "    0, 0, 0, 1    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GateApproxEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    A Gym environment for reinforcement‐learning synthesis of two‐qubit braid gates\n",
    "    in the Fibonacci anyon model.\n",
    "\n",
    "    The agent composes a sequence of braid generators (from `braid_gates`) up to `max_length`\n",
    "    to approximate a target two‐qubit gate `target_gate`. Rewards penalize leakage, closeness\n",
    "    error, and unitarity error via weights (alpha, beta, gamma). Optionally measures local \n",
    "    equivalence with Makhlin invariants.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, braid_gates, target_gate, subs, max_length,\n",
    "                 alpha, beta, gamma, local_equivalence_class):\n",
    "        \"\"\"\n",
    "        Initialize the GateApproxEnv.\n",
    "\n",
    "        :param braid_gates: list of symbolic SymPy matrices for braid generators\n",
    "        :param target_gate: symbolic SymPy matrix for the desired two-qubit gate\n",
    "        :param subs: dictionary of substitutions for sympy symbols → numeric values\n",
    "        :param max_length: maximum braid sequence length per episode\n",
    "        :param alpha: weight for leakage penalty\n",
    "        :param beta: weight for closeness penalty\n",
    "        :param gamma: weight for unitarity penalty\n",
    "        :param local_equivalence_class: if True, use Makhlin invariants for closeness\n",
    "        \"\"\"\n",
    "        super(GateApproxEnv, self).__init__()\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.local_equivalence_class = local_equivalence_class\n",
    "\n",
    "        # Precompute numeric versions of braid_gates\n",
    "        self.braid_gates = []\n",
    "        for g in braid_gates:\n",
    "            g_evaluated = g.subs(subs).evalf()\n",
    "            if any(sym.is_symbol for sym in g_evaluated):\n",
    "                raise ValueError(\"Not all symbols substituted in a braid gate.\")\n",
    "            self.braid_gates.append(np.array(g_evaluated.tolist(), dtype=complex))\n",
    "\n",
    "        # Precompute numeric target gate\n",
    "        t_evaluated = target_gate.subs(subs).evalf()\n",
    "        if any(sym.is_symbol for sym in t_evaluated):\n",
    "            raise ValueError(\"Not all symbols substituted in target_gate.\")\n",
    "        self.target_gate = np.array(t_evaluated.tolist(), dtype=complex)\n",
    "\n",
    "        # Action space: One action per braid gate\n",
    "        self.action_space = spaces.Discrete(len(self.braid_gates))\n",
    "\n",
    "        # Observation: Flattened real+imag parts of the 5x5 gate = 50-dim vector\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(50,), dtype=np.float64)\n",
    "\n",
    "        self.reset_composition()\n",
    "\n",
    "    def reset_composition(self):\n",
    "        \"\"\"Reset gate composition and stack for a new episode.\"\"\"\n",
    "        self.current_composition = np.eye(5, dtype=complex)\n",
    "        self.current_length = 0\n",
    "        self.gate_stack = []\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        \"\"\"\n",
    "        Start a new episode.\n",
    "\n",
    "        Randomizes episode length, resets composition, and stores previous error metrics\n",
    "        for reward shaping.\n",
    "        \"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        self.reset_composition()\n",
    "        self.current_step = 0\n",
    "\n",
    "        # Randomize the episode length \n",
    "        self.random_episode_length = random.randint(20, self.max_length)\n",
    "\n",
    "        # Compute initial errors (for reward shaping)\n",
    "        leak_err, uni_err, close_err, _ = self.compute_reward()\n",
    "        self.prev_leakage = leak_err\n",
    "        self.prev_unitarity_error = uni_err\n",
    "        self.prev_closeness_error = close_err\n",
    "\n",
    "        # Store a \"prev total error\" to measure improvement\n",
    "        self.prev_total_error = (self.alpha * leak_err \n",
    "                                 + self.beta * close_err \n",
    "                                 + self.gamma * uni_err)\n",
    "\n",
    "        return self._get_obs(), {}\n",
    "    \n",
    "    def take_action(self, action):\n",
    "        \"\"\"\n",
    "        Apply braid gate corresponding to `action` index to current composition.\n",
    "        Truncates if max_length is reached.\n",
    "        \"\"\"\n",
    "        if self.current_length < self.max_length:\n",
    "            # Numeric matrix multiplication only\n",
    "            self.current_composition = self.current_composition @ self.braid_gates[action]\n",
    "            self.gate_stack.append(f\"{action}\")\n",
    "            self.current_length += 1\n",
    "        else:\n",
    "            print(\"Warning: Maximum composition length reached. No action taken.\")\n",
    "\n",
    "    def compute_reward(self):\n",
    "        \"\"\"\n",
    "        Compute leakage, closeness error, unitarity error, and combined reward.\n",
    "        :returns: (leakage, closeness_error, unitarity_error, reward)\n",
    "        \"\"\"\n",
    "        # Current composition is already numeric\n",
    "        M = self.current_composition\n",
    "        T = self.target_gate\n",
    "\n",
    "        # Leakage: abs(M[0,0])\n",
    "        leakage = np.abs(M[0,0])\n",
    "        # Avoid dividing by zero if leakage is extremely small:\n",
    "        if leakage < 1e-12:\n",
    "            leakage = 1e-12\n",
    "        #leakage_error = 1.0 / leakage\n",
    "\n",
    "        leakage_error = (1.0 - leakage)\n",
    "\n",
    "        # Extract 4x4 submatrix\n",
    "        M_4x4 = M[1:5, 1:5]\n",
    "\n",
    "        # Unitarity check\n",
    "        UdagU = M_4x4.conjugate().T @ M_4x4\n",
    "        unitarity_error = self.schatten_p_norm(UdagU - np.eye(4), 1)\n",
    "\n",
    "        # Closeness to target\n",
    "        if self.local_equivalence_class:\n",
    "            closeness_error = self.local_equivalence_distance(T, M_4x4)\n",
    "        else:\n",
    "            # Schatten 2-norm difference between normalized gates\n",
    "            A_norm = self.schatten_p_norm(M_4x4, 2)\n",
    "            T_norm = self.schatten_p_norm(T, 2)\n",
    "\n",
    "            if A_norm < 1e-12:  # safeguard\n",
    "                A_norm = 1e-12\n",
    "            if T_norm < 1e-12:\n",
    "                T_norm = 1e-12\n",
    "\n",
    "            A_normalized = M_4x4 / A_norm\n",
    "            T_normalized = T / T_norm\n",
    "            closeness_error = self.schatten_p_norm(A_normalized - T_normalized, 2)\n",
    "\n",
    "        # Reward (negative weighted sum)\n",
    "        reward = - (self.alpha * leakage_error + self.beta * closeness_error + self.gamma * unitarity_error)\n",
    "        \n",
    "        return float(leakage), float(closeness_error), float(unitarity_error), float(reward)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Gym step: apply action, compute incremental reward, and return new state.\n",
    "        On termination, include final metrics and Makhlin invariants in info.\n",
    "        \"\"\"\n",
    "        old_total_error = self.prev_total_error\n",
    "\n",
    "        # 2. Take action\n",
    "        self.take_action(action)\n",
    "        self.current_step += 1\n",
    "\n",
    "        # 3. Compute new error\n",
    "        leakage, closeness_error, unitarity_error, combined_reward = self.compute_reward()\n",
    "        new_total_error = (self.alpha * (1.0 - leakage) + self.beta * closeness_error + self.gamma * unitarity_error)\n",
    "\n",
    "        # 4. Provide incremental reward for improvement\n",
    "        # If we reduced total_error, that's an improvement => positive reward\n",
    "        improvement = old_total_error - new_total_error\n",
    "        step_reward = improvement \n",
    "\n",
    "        # Update stored \"previous\" values for next step\n",
    "        self.prev_leakage = leakage\n",
    "        self.prev_unitarity_error = unitarity_error\n",
    "        self.prev_closeness_error = closeness_error\n",
    "        self.prev_total_error = new_total_error\n",
    "\n",
    "        terminated = (self.current_step >= self.random_episode_length)\n",
    "        truncated = False\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        # If the episode terminates, add a final reward that reflects\n",
    "        # the final composition's closeness to target\n",
    "        if terminated:\n",
    "            step_reward += combined_reward\n",
    "\n",
    "            # Compute final 4x4 submatrix and Makhlin invariants:\n",
    "            M_4x4 = self.current_composition[1:5, 1:5]\n",
    "            g_1, g_2, g_3 = self.compute_makhlin_invariants(M_4x4)\n",
    "\n",
    "            info = {\n",
    "                \"current_step\": self.current_step,\n",
    "                \"gate_stack\": self.gate_stack.copy(),\n",
    "                \"leakage\": leakage,\n",
    "                \"closeness_error\": closeness_error,\n",
    "                \"unitarity_error\": unitarity_error,\n",
    "                \"combined_reward\": combined_reward,\n",
    "                \"improvement\": improvement,\n",
    "                \"final_reward\": step_reward if terminated else None,\n",
    "                \"g_1\":g_1,\n",
    "                \"g_2\":g_2,\n",
    "                \"g_3\":g_3,\n",
    "                \"M_4x4\": M_4x4\n",
    "            }\n",
    "\n",
    "        obs = self._get_obs()\n",
    "\n",
    "        return obs, step_reward, terminated, truncated, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\"Return the flattened real+imag parts of the current 5×5 composition.\"\"\"\n",
    "        # current_composition is numeric, just flatten\n",
    "        M = self.current_composition\n",
    "        obs = np.concatenate([M.real.flatten(), M.imag.flatten()])\n",
    "        return obs\n",
    "\n",
    "    \"\"\"\n",
    "    def schatten_p_norm(self, T, p):\n",
    "\n",
    "        # Compute |T| = sqrt(T^† T)\n",
    "        abs_T = sqrtm(T.conj().T @ T)\n",
    "\n",
    "        # Compute |T|^p\n",
    "        abs_T_p = np.linalg.matrix_power(abs_T, p)\n",
    "\n",
    "        # Compute the trace of |T|^p\n",
    "        trace_value = np.trace(abs_T_p)\n",
    "\n",
    "        # Compute the Schatten p-norm\n",
    "        schatten_norm = np.real(trace_value)**(1/p)\n",
    "\n",
    "        return schatten_norm\n",
    "    \"\"\"\n",
    "\n",
    "    def schatten_p_norm(self, T, p):\n",
    "        \"\"\"Compute the Schatten p-norm of matrix T.\"\"\"\n",
    "        # Ensure T is a NumPy array\n",
    "        T = np.array(T, dtype=complex)\n",
    "\n",
    "        # Compute singular values of T\n",
    "        singular_values = np.linalg.svd(T, compute_uv=False)\n",
    "\n",
    "        # Compute Schatten p-norm\n",
    "        if p == np.inf:  # Special case for p = infinity\n",
    "            schatten_norm = np.max(singular_values)\n",
    "        elif p == 1:  # Special case for p = 1 (nuclear norm)\n",
    "            schatten_norm = np.sum(singular_values)\n",
    "        else:\n",
    "            # General case for arbitrary p\n",
    "            schatten_norm = (np.sum(singular_values**p))**(1/p)\n",
    "\n",
    "        return schatten_norm\n",
    "\n",
    "    def compute_makhlin_invariants(self, U):\n",
    "        \"\"\"\n",
    "        Calculate the three Makhlin invariants (g1, g2, g3) for a 4×4 unitary U.\n",
    "        \"\"\"\n",
    "\n",
    "        i = 1j  # Complex unit (sqrt(-1))\n",
    "        Q = (1 / np.sqrt(2)) * np.array([\n",
    "            [1,  0,  0,  i],\n",
    "            [0,  i,  1,  0],\n",
    "            [0,  i, -1,  0],\n",
    "            [1,  0,  0, -i]\n",
    "        ], dtype=complex)\n",
    "\n",
    "        # Compute U_B = Q^\\dagger U Q\n",
    "        U_B = Q.conjugate().T @ U @ Q\n",
    "\n",
    "        # Makhlin matrix: m_U = (U_B)^T U_B\n",
    "        m_U = (U_B.T) @ U_B\n",
    "\n",
    "        # Compute trace and related quantities\n",
    "        tr_mU = np.trace(m_U)\n",
    "        tr_mU2 = np.trace(m_U @ m_U)\n",
    "        det_U = np.linalg.det(U)  # determinant of U\n",
    "\n",
    "        if np.abs(det_U) < 1e-12:\n",
    "            print(\"Warning: Determinant is very small, adding regularization.\")\n",
    "            det_U += 1e-12\n",
    "\n",
    "\n",
    "        # Compute complex quantity: (tr^2(m_U) / (16 * det(U)))\n",
    "        complex_val = (tr_mU**2) / (16.0 * det_U)\n",
    "\n",
    "        # g_1 = Re{complex_val}\n",
    "        g_1 = complex_val.real\n",
    "\n",
    "        # g_2 = Im{complex_val}\n",
    "        g_2 = complex_val.imag\n",
    "\n",
    "        # g_3 = (tr^2(m_U) - tr(m_U^2)) / (4 * det(U))\n",
    "        g_3 = ((tr_mU**2) - tr_mU2) / (4.0 * det_U)\n",
    "\n",
    "        return (g_1, g_2, g_3)\n",
    "\n",
    "    def local_equivalence_distance(self, E, U):\n",
    "        \"\"\"\n",
    "        Compute squared‐distance between Makhlin invariants of target E and candidate U.\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute Makhlin invariants for E and U\n",
    "        gE = self.compute_makhlin_invariants(E)\n",
    "        gU = self.compute_makhlin_invariants(U)\n",
    "\n",
    "        # Compute Δg_i and sum their squares\n",
    "        diff_squares = [(abs(e - u))**2 for e, u in zip(gE, gU)]\n",
    "        d_EU = sum(diff_squares)\n",
    "        return d_EU\n",
    "    \n",
    "\n",
    "\n",
    "    def get_gate_composition(self, gate_string: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Given a string of digits representing gate indices, compute the total 5×5 matrix.\n",
    "        Raises ValueError or IndexError on invalid characters or indices.\n",
    "        \"\"\"\n",
    "        composition = np.eye(5, dtype=complex)\n",
    "        for char in gate_string:\n",
    "            try:\n",
    "                gate_index = int(char)\n",
    "            except ValueError:\n",
    "                raise ValueError(f\"Invalid character '{char}' in gate string. Must be a digit.\")\n",
    "\n",
    "            if gate_index < 0 or gate_index >= len(self.braid_gates):\n",
    "                raise IndexError(f\"Gate index {gate_index} is out of bounds for the available braid gates.\")\n",
    "\n",
    "            composition = composition @ self.braid_gates[gate_index]\n",
    "        return composition\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "#braid_gates = [rho_1, rho_2, rho_3, rho_4, rho_5]\n",
    "\n",
    "\n",
    "braid_gates = [\n",
    "        rho_1,       rho_2,       rho_3,       rho_4,       rho_5,\n",
    "        rho_1.inv(), rho_2.inv(), rho_3.inv(), rho_4.inv(), rho_5.inv()\n",
    "    ]\n",
    "\n",
    "\n",
    "target_gate = cnot_gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gate_approx_env(env_kwargs):\n",
    "    def _init():\n",
    "        env = GateApproxEnv(**env_kwargs)\n",
    "  \n",
    "        # env.seed(seed + rank)  \n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "env_kwargs = {\n",
    "    \"braid_gates\": braid_gates,        \n",
    "    \"target_gate\": target_gate,        \n",
    "    \"subs\": subs,                      \n",
    "    \"max_length\": SEQUENCE_LENGTH,                  \n",
    "    \"alpha\": ALPHA,\n",
    "    \"beta\": BETA,\n",
    "    \"gamma\": GAMMA,\n",
    "    \"local_equivalence_class\": True\n",
    "}\n",
    "\n",
    "num_envs = 8  # number of parallel processes\n",
    "\n",
    "# Build a list of environment-initializer callables\n",
    "env_fns = [make_gate_approx_env(env_kwargs)\n",
    "           for i in range(num_envs)]\n",
    "\n",
    "# Create the SubprocVecEnv\n",
    "vec_env = SubprocVecEnv(env_fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_timesteps = 100\n",
    "verbose = 1\n",
    "\n",
    "policy_kwargs = dict(net_arch=[512, 256, 128, 64])\n",
    "model = RecurrentPPO(\n",
    "    \"MlpLstmPolicy\",\n",
    "    vec_env,\n",
    "    #env,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    #device=\"cuda\",\n",
    "    verbose=verbose,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    gamma=0.9,\n",
    "    tensorboard_log=\"./tensorboard_logs/\"\n",
    "    )\n",
    "\n",
    "info_callback = InfoCallback()\n",
    "model.learn(total_timesteps=total_timesteps, callback=info_callback)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_metrics(callback: InfoCallback):\n",
    "    \n",
    "    episodes = range(1, len(callback.episode_rewards) + 1)\n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n",
    "\n",
    "    \n",
    "\n",
    "    # Episode Final Reward\n",
    "    axs[0, 0].plot(episodes, callback.episode_rewards, label='Final Episode Reward')\n",
    "    axs[0, 0].set_xlabel('Episode')\n",
    "    axs[0, 0].set_ylabel('Reward')\n",
    "    axs[0, 0].set_title('Episode Final Rewards')\n",
    "    axs[0, 0].legend()\n",
    "\n",
    "    # Leakage\n",
    "    axs[0, 1].plot(episodes, callback.leakage_log, color='orange', label='Leakage')\n",
    "    axs[0, 1].set_xlabel('Episode')\n",
    "    axs[0, 1].set_ylabel('Leakage')\n",
    "    axs[0, 1].set_title('Leakage per Episode')\n",
    "    axs[0, 1].legend()\n",
    "\n",
    "    # Closeness Error\n",
    "    axs[1, 0].plot(episodes, callback.closeness_log, color='green', label='Closeness Error')\n",
    "    axs[1, 0].set_xlabel('Episode')\n",
    "    axs[1, 0].set_ylabel('Closeness Error')\n",
    "    axs[1, 0].set_title('Closeness per Episode')\n",
    "    axs[1, 0].legend()\n",
    "\n",
    "    # Unitarity Error\n",
    "    axs[1, 1].plot(episodes, callback.unitarity_log, color='red', label='Unitarity Error')\n",
    "    axs[1, 1].set_xlabel('Episode')\n",
    "    axs[1, 1].set_ylabel('Unitarity Error')\n",
    "    axs[1, 1].set_title('Unitarity per Episode')\n",
    "    axs[1, 1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the main metrics\n",
    "plot_training_metrics(info_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_low_unitary_acceleration(callback, threshold=0.001, increment=1.0):\n",
    "    # Convert the unitarity log into a NumPy array.\n",
    "    errors = np.array(callback.unitarity_log)\n",
    "    num_episodes = len(errors)\n",
    "    \n",
    "    # Initialize the cumulative array.\n",
    "    cumulative = np.zeros(num_episodes)\n",
    "    \n",
    "    # For each episode, if the unitarity error is below threshold,\n",
    "    # subtract 'decrement' from the cumulative value.\n",
    "    for i in range(num_episodes):\n",
    "        if i > 0:\n",
    "            cumulative[i] = cumulative[i-1]\n",
    "        if errors[i] < threshold:\n",
    "            cumulative[i] += increment\n",
    "\n",
    "    episodes = np.arange(1, num_episodes + 1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(episodes, cumulative, marker='.', linestyle='-')\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Cumulative Low-Error Score\")\n",
    "    plt.title(\"Acceleration of Achieving Low Unitarity Error\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_low_leakage_acceleration(callback, threshold=0.9, increment=1.0):\n",
    "    # Get leakage errors as a NumPy array.\n",
    "    leakage_errors = np.array(callback.leakage_log)\n",
    "    num_episodes = len(leakage_errors)\n",
    "    \n",
    "    # Initialize the cumulative score array.\n",
    "    cumulative = np.zeros(num_episodes)\n",
    "    \n",
    "    # For each episode, if leakage is below the threshold, decrement the cumulative score.\n",
    "    for i in range(num_episodes):\n",
    "        if i > 0:\n",
    "            cumulative[i] = cumulative[i-1]\n",
    "        if leakage_errors[i] > threshold:\n",
    "            cumulative[i] += increment\n",
    "\n",
    "    episodes = np.arange(1, num_episodes + 1)\n",
    "    \n",
    "    # Plot the cumulative score.\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(episodes, cumulative, marker='.', linestyle='-')\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Cumulative Low-Leakage Score\")\n",
    "    plt.title(\"Acceleration of Achieving Low Leakage\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_low_unitary_acceleration(info_callback, threshold=0.0000000000001, increment=1.0)\n",
    "\n",
    "plot_low_leakage_acceleration(info_callback, threshold=0.95, increment=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy_vec(env, model, num_sequences, csv_filename=\"results_lec.csv\"):\n",
    "    \"\"\"\n",
    "    Run a trained RL policy in a vectorized environment, collect episode metrics,\n",
    "    and export those sequences meeting fidelity and leakage thresholds to CSV.\n",
    "\n",
    "    The function:\n",
    "    1. Resets the VecEnv.\n",
    "    2. Rolls out the policy stochastically for `num_sequences` episodes.\n",
    "    3. For each completed episode, extracts closeness error, leakage, unitarity error,\n",
    "       and the braid sequence (`gate_stack`) from `infos`.\n",
    "    4. Writes only those episodes with closeness_error < 1e-4 and leakage > 0.98\n",
    "       to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        env: A vectorized Gym environment (e.g., SubprocVecEnv) implementing\n",
    "             reset() -> obs and step(actions) -> (obs, rewards, dones, infos).\n",
    "        model: A Stable-Baselines3 policy with .predict(obs, deterministic=False).\n",
    "        num_sequences: Total number of episodes to sample.\n",
    "        csv_filename: Path for output CSV. Default is \"results_lec.csv\".\n",
    "\n",
    "    Returns:\n",
    "        None. Outputs CSV file and prints a confirmation message.\n",
    "    \"\"\"\n",
    "    \n",
    "    SEQUENCE_LENGTH = getattr(env, 'max_length', 10)\n",
    "    max_operator_width = SEQUENCE_LENGTH\n",
    "    operator_column_width = max(max_operator_width, 10)\n",
    "\n",
    "    # Open CSV file for writing\n",
    "    with open(csv_filename, \"w\", newline=\"\") as csvfile:\n",
    "        fieldnames = [\"Closeness\", \"Leakage\", \"Unitarity\", \"Operator\"]\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        # Reset all parallel environments at once\n",
    "        obs = env.reset()  # Assuming single return value\n",
    "        episodes_collected = 0\n",
    "\n",
    "        while episodes_collected < num_sequences:\n",
    "            actions, _ = model.predict(obs, deterministic=False)\n",
    "            obs, _, dones, infos = env.step(actions)\n",
    "\n",
    "            for i in range(env.num_envs):\n",
    "                if dones[i]:\n",
    "                    episodes_collected += 1\n",
    "                    gate_stack = infos[i].get(\"gate_stack\", [])\n",
    "                    operator_string = ''.join(map(str, gate_stack))\n",
    "\n",
    "                    closeness_error = infos[i].get(\"closeness_error\", 9999)\n",
    "                    leakage = infos[i].get(\"leakage\", 0.0)\n",
    "                    unitarity_error = infos[i].get(\"unitarity_error\", 9999)\n",
    "\n",
    "                    # Retrieve Makhlin invariants if needed:\n",
    "                    # g_1 = infos[i].get(\"g_1\", float('nan'))\n",
    "                    # g_2 = infos[i].get(\"g_2\", float('nan'))\n",
    "                    # g_3 = infos[i].get(\"g_3\", float('nan'))\n",
    "\n",
    "                    # Output row only if the condition is met:\n",
    "                    if closeness_error < .0001 and (leakage > 0.98):\n",
    "                    #if (closeness_error < 0.9) and (leakage > 0.8) and (unitarity_error < 1e-10):\n",
    "                        writer.writerow({\n",
    "                            \"Closeness\": f\"{closeness_error:.4e}\",\n",
    "                            \"Leakage\": f\"{leakage:.3f}\",\n",
    "                            \"Unitarity\": f\"{unitarity_error:.3e}\",\n",
    "                            \"Operator\": operator_string\n",
    "                        })\n",
    "\n",
    "                    if episodes_collected >= num_sequences:\n",
    "                        break\n",
    "\n",
    "            if episodes_collected >= num_sequences:\n",
    "                break\n",
    "\n",
    "    print(f\"Results exported to {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_policy_vec_display(env, model, num_sequences, \n",
    "                                closeness_thresh=0.1, leakage_thresh=0.95):\n",
    "    \"\"\"\n",
    "    Run the policy in a VecEnv for num_sequences episodes, collect gates meeting\n",
    "    given thresholds, and display them in a formatted pandas DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - env: VecEnv instance\n",
    "    - model: trained SB3 policy (PPO)\n",
    "    - num_sequences: total episodes to sample\n",
    "    - closeness_thresh: only include gates with closeness_error < this\n",
    "    - leakage_thresh: only include gates with leakage > this\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with columns ['Operator', 'Length', 'Closeness', 'Leakage', 'Unitarity']\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    episodes = 0\n",
    "    \n",
    "    # Reset env\n",
    "    ret = env.reset()\n",
    "    obs = ret[0] if isinstance(ret, (tuple, list)) else ret\n",
    "    \n",
    "    while episodes < num_sequences:\n",
    "        actions, _ = model.predict(obs, deterministic=False)\n",
    "        step_out = env.step(actions)\n",
    "        # unpack\n",
    "        if len(step_out) == 5:\n",
    "            obs, _, dones, truncs, infos = step_out\n",
    "        else:\n",
    "            obs, _, dones, infos = step_out\n",
    "            truncs = dones\n",
    "        \n",
    "        for i in range(env.num_envs):\n",
    "            if dones[i] or truncs[i]:\n",
    "                episodes += 1\n",
    "                info = infos[i]\n",
    "                closeness = info.get('closeness_error', float('nan'))\n",
    "                leakage   = info.get('leakage', float('nan'))\n",
    "                unitarity = info.get('unitarity_error', float('nan'))\n",
    "                if closeness < closeness_thresh and leakage > leakage_thresh:\n",
    "                    op = ''.join(map(str, info.get('gate_stack', [])))\n",
    "                    records.append({\n",
    "                        'Operator': op,\n",
    "                        'Length': len(op),\n",
    "                        'Closeness': closeness,\n",
    "                        'Leakage': leakage,\n",
    "                        'Unitarity': unitarity\n",
    "                    })\n",
    "                # reset this sub-env\n",
    "                single_ret = env.env_method(\"reset\", indices=[i])[0]\n",
    "                obs_i = single_ret[0] if isinstance(single_ret, (tuple, list)) else single_ret\n",
    "                obs[i] = obs_i\n",
    "                \n",
    "                if episodes >= num_sequences:\n",
    "                    break\n",
    "    \n",
    "    # Build DataFrame\n",
    "    df = pd.DataFrame(records)\n",
    "    if df.empty:\n",
    "        print(\"No gates met the thresholds.\")\n",
    "        return df\n",
    "    \n",
    "    # Sort by Closeness ascending, then by Length ascending\n",
    "    df = df.sort_values(['Closeness','Length']).reset_index(drop=True)\n",
    "    \n",
    "    # Display\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    print(f\"\\nConstructed gates meeting closeness<{closeness_thresh}, leakage>{leakage_thresh}:\")\n",
    "    display(df[['Operator','Length','Closeness','Leakage','Unitarity']])\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = PPO.load(\"working_model\", env=vec_env) # 21 to 40\n",
    "\n",
    "#loaded_model = PPO.load(\"length_3_inv_model\", env=vec_env)\n",
    "#loaded_model = PPO.load(\"length_4_inv_model\", env=vec_env)\n",
    "#loaded_model = PPO.load(\"length_5_inv_model\", env=vec_env)\n",
    "#loaded_model = PPO.load(\"length_6_inv_model\", env=vec_env)\n",
    "#loaded_model = PPO.load(\"length_7_inv_model\", env=vec_env)\n",
    "\n",
    "#loaded_model = PPO.load(\"length_10_inv_model\", env=vec_env)\n",
    "#loaded_model = PPO.load(\"length_20_inv_model\", env=vec_env)\n",
    "#loaded_model = PPO.load(\"length_21_inv_model\", env=vec_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_policy_vec_display(vec_env, loaded_model, num_sequences=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save(\"length_20_inv_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welsh t-test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_gate_metrics(env, model, num_sequences,\n",
    "                         closeness_thresh=1.0, leakage_thresh=0.8):\n",
    "    \"\"\"\n",
    "    Run the policy in a VecEnv for num_sequences episodes, collect metrics into a list.\n",
    "    Returns: list of dicts with keys 'Operator','Length','Closeness','Leakage','Unitarity'\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    episodes = 0\n",
    "    ret = env.reset()\n",
    "    obs = ret[0] if isinstance(ret, (tuple,list)) else ret\n",
    "\n",
    "    while episodes < num_sequences:\n",
    "        # get actions (stochastic)\n",
    "        actions, _ = model.predict(obs, deterministic=False)\n",
    "        step_out = env.step(actions)\n",
    "        if len(step_out) == 5:\n",
    "            obs, _, dones, truncs, infos = step_out\n",
    "        else:\n",
    "            obs, _, dones, infos = step_out\n",
    "            truncs = dones\n",
    "\n",
    "        # inspect each sub-environment\n",
    "        for i in range(env.num_envs):\n",
    "            if dones[i] or truncs[i]:\n",
    "                info = infos[i]\n",
    "                closeness = info.get('closeness_error', np.nan)\n",
    "                leakage   = info.get('leakage', np.nan)\n",
    "                unitarity = info.get('unitarity_error', np.nan)\n",
    "                if (closeness < closeness_thresh) and (leakage > leakage_thresh):\n",
    "                    op = ''.join(map(str, info.get('gate_stack', [])))\n",
    "                    records.append({\n",
    "                        'Operator': op,\n",
    "                        'Length': len(op),\n",
    "                        'Closeness': closeness,\n",
    "                        'Leakage': leakage,\n",
    "                        'Unitarity': unitarity\n",
    "                    })\n",
    "                episodes += 1\n",
    "                # reset this sub-env\n",
    "                single_ret = env.env_method(\"reset\", indices=[i])[0]\n",
    "                obs_i = single_ret[0] if isinstance(single_ret, (tuple,list)) else single_ret\n",
    "                obs[i] = obs_i\n",
    "                if episodes >= num_sequences:\n",
    "                    break\n",
    "    return records\n",
    "\n",
    "def plot_metric_bars(records, metric):\n",
    "    \"\"\"\n",
    "    Given a list of records and a metric name, plot a bar chart of metric values.\n",
    "    X-axis: record index, Y-axis: metric value.\n",
    "    \"\"\"\n",
    "    values = [r[metric] for r in records]\n",
    "    indices = list(range(len(values)))\n",
    "    plt.figure()\n",
    "    plt.bar(indices, values)\n",
    "    plt.xlabel('Gate Rank')\n",
    "    plt.ylabel(metric)\n",
    "    plt.title(f'Bar Chart of {metric} for Collected Gates')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def analyze_and_plot(env, model, num_sequences=500,\n",
    "                     closeness_thresh=1.0, leakage_thresh=0.8):\n",
    "    \"\"\"\n",
    "    Collect gate metrics and plot bar charts for Closeness, Leakage, Unitarity, Length.\n",
    "    \"\"\"\n",
    "    records = collect_gate_metrics(env, model, num_sequences,\n",
    "                                   closeness_thresh, leakage_thresh)\n",
    "    if not records:\n",
    "        print(\"No records met thresholds.\")\n",
    "        return\n",
    "    # Convert to DataFrame for display if desired\n",
    "    df = pd.DataFrame(records)\n",
    "    print(df)\n",
    "    # Plot each metric\n",
    "    for metric in ['Closeness', 'Leakage', 'Unitarity', 'Length']:\n",
    "        plot_metric_bars(records, metric)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "df = analyze_and_plot(vec_env, loaded_model, num_sequences=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_random_sequence_metrics(env, gate_strings,\n",
    "                                    closeness_thresh=1.0,\n",
    "                                    leakage_thresh=0.8,\n",
    "                                    csv_filename=\"random_gate_metrics.csv\"):\n",
    "    \"\"\"\n",
    "    Given a list of random gate strings, compute their metrics, print a formatted table,\n",
    "    write to CSV, and return a list of metric dicts for those passing thresholds.\n",
    "\n",
    "    Parameters:\n",
    "    - env: GateApproxEnv with methods `reset_composition`, `get_gate_composition`, `compute_reward`\n",
    "    - gate_strings: list of str, each a sequence of gate indices\n",
    "    - closeness_thresh: float, include only if closeness_error < this\n",
    "    - leakage_thresh: float, include only if leakage > this\n",
    "    - csv_filename: str, output CSV filename\n",
    "\n",
    "    Returns:\n",
    "    - records: list of dicts with keys ['Operator','Length','Closeness','Leakage','Unitarity']\n",
    "    \"\"\"\n",
    "    # Determine column width\n",
    "    max_op = max(len(s) for s in gate_strings)\n",
    "    width = max(max_op, len(\"Operator\"), 10)\n",
    "\n",
    "    # Print header\n",
    "    header = f\"{'Operator':<{width}} | {'Closeness':<10} | {'Leakage':<10} | {'Unitarity':<10}\"\n",
    "    print(header)\n",
    "    print(\"-\" * (width + 34))\n",
    "\n",
    "    # Prepare CSV\n",
    "    with open(csv_filename, \"w\", newline=\"\") as csvfile:\n",
    "        fieldnames = [\"Operator\", \"Closeness\", \"Leakage\", \"Unitarity\"]\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        records = []\n",
    "        for s in gate_strings:\n",
    "            # Reset and build unitary\n",
    "            env.reset_composition()\n",
    "            gate_matrix = env.get_gate_composition(s)\n",
    "            env.current_composition = gate_matrix\n",
    "\n",
    "            # Compute metrics\n",
    "            leakage, closeness_error, unitarity_error, _ = env.compute_reward()\n",
    "\n",
    "            # Threshold filter\n",
    "            if closeness_error >= closeness_thresh or leakage <= leakage_thresh:\n",
    "                continue\n",
    "\n",
    "            # Format and print\n",
    "            print(f\"{s:<{width}} | {closeness_error:<10.3e} | {leakage:<10.3f} | {unitarity_error:<10.3e}\")\n",
    "\n",
    "            # Write CSV\n",
    "            writer.writerow({\n",
    "                \"Operator\": s,\n",
    "                \"Closeness\": f\"{closeness_error:.3e}\",\n",
    "                \"Leakage\": f\"{leakage:.3f}\",\n",
    "                \"Unitarity\": f\"{unitarity_error:.3e}\"\n",
    "            })\n",
    "\n",
    "            # Append to output list\n",
    "            records.append({\n",
    "                \"Operator\":  s,\n",
    "                \"Length\":    len(s),\n",
    "                \"Closeness\": closeness_error,\n",
    "                \"Leakage\":   leakage,\n",
    "                \"Unitarity\": unitarity_error\n",
    "            })\n",
    "\n",
    "    print(f\"\\nMetrics written to {csv_filename}\")\n",
    "    return records\n",
    "\n",
    "\n",
    "def generate_random_gate_string(min_length: int, max_length: int, num_gates: int) -> str:\n",
    "    \"\"\"Generate a single random gate string of random length between min_length and max_length.\"\"\"\n",
    "    length = random.randint(min_length, max_length)\n",
    "    return ''.join(str(random.randrange(num_gates)) for _ in range(length))\n",
    "\n",
    "def generate_random_gate_strings(count: int, min_length: int, max_length: int, num_gates: int) -> list:\n",
    "    \"\"\"Generate multiple random gate strings.\"\"\"\n",
    "    return [generate_random_gate_string(min_length, max_length, num_gates) for _ in range(count)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GateApproxEnv(\n",
    "    braid_gates=braid_gates,\n",
    "    target_gate=target_gate,\n",
    "    subs=subs,\n",
    "    max_length=SEQUENCE_LENGTH,\n",
    "    alpha=ALPHA,\n",
    "    beta=BETA,\n",
    "    gamma=GAMMA,\n",
    "    local_equivalence_class=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models_statistics(model_data, random_data, metrics=None):\n",
    "    \"\"\"\n",
    "    Statistically compare model vs random using independent t-tests and visualize.\n",
    "    Accepts either lists of dicts or pandas DataFrames for inputs.\n",
    "    \n",
    "    Parameters:\n",
    "    - model_data: list of dicts or DataFrame with keys/columns ['Closeness','Leakage','Unitarity','Length']\n",
    "    - random_data: same as model_data\n",
    "    - metrics: list of metric names to compare; defaults to ['Closeness','Leakage','Unitarity','Length']\n",
    "    \n",
    "    Returns:\n",
    "    - summary_df: DataFrame with mean±std for each strategy and metric\n",
    "    - p_values: dict of p-values from t-tests for each metric\n",
    "    \"\"\"\n",
    "    # Convert to DataFrames if necessary\n",
    "    if isinstance(model_data, list):\n",
    "        df_model = pd.DataFrame(model_data)\n",
    "    else:\n",
    "        df_model = model_data.copy()\n",
    "    if isinstance(random_data, list):\n",
    "        df_rand = pd.DataFrame(random_data)\n",
    "    else:\n",
    "        df_rand = random_data.copy()\n",
    "    \n",
    "    # Ensure required columns exist\n",
    "    if metrics is None:\n",
    "        metrics = ['Closeness','Leakage','Unitarity','Length']\n",
    "    for df, name in [(df_model, \"Model\"), (df_rand, \"Random\")]:\n",
    "        missing = [m for m in metrics if m not in df.columns]\n",
    "        if missing:\n",
    "            raise KeyError(f\"{name} data is missing columns: {missing}\")\n",
    "    \n",
    "    # Tag strategies\n",
    "    df_model['strategy'] = 'Model'\n",
    "    df_rand['strategy']  = 'Random'\n",
    "    df = pd.concat([df_model, df_rand], ignore_index=True)\n",
    "    \n",
    "    # Summary statistics\n",
    "    summary = df.groupby('strategy')[metrics].agg(['mean','std']).round(4)\n",
    "    \n",
    "    # Independent two-sample t-tests (Welch's)\n",
    "    p_values = {}\n",
    "    for metric in metrics:\n",
    "        m_vals = df_model[metric].dropna()\n",
    "        r_vals = df_rand[metric].dropna()\n",
    "        t_stat, p_val = stats.ttest_ind(m_vals, r_vals, equal_var=False)\n",
    "        p_values[metric] = p_val\n",
    "    \n",
    "    # Display summary and p-values\n",
    "    print(\"=== Summary Statistics ===\")\n",
    "    display(summary)\n",
    "    print(\"\\n=== p-values (Model vs Random) ===\")\n",
    "    for metric, p in p_values.items():\n",
    "        print(f\"{metric}: p = {p:.4e}\")\n",
    "    \n",
    "    # Boxplots\n",
    "    fig, axes = plt.subplots(1, len(metrics), figsize=(4*len(metrics), 5))\n",
    "    if len(metrics) == 1:\n",
    "        axes = [axes]\n",
    "    for ax, metric in zip(axes, metrics):\n",
    "        ax.boxplot([df_model[metric].dropna(), df_rand[metric].dropna()], labels=['Model','Random'])\n",
    "        ax.set_title(metric)\n",
    "        ax.set_ylabel(metric)\n",
    "    fig.suptitle(\"Model vs Random: Metric Distributions\")\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "    \n",
    "    return summary, p_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters = 1000\n",
    "\n",
    "random_records = collect_random_metrics(env, num_sequences=num_iters,\n",
    "                                        min_length=21, max_length=40,\n",
    "                                        closeness_thresh=1.0, leakage_thresh=0.8)\n",
    "df_random = pd.DataFrame(random_records)\n",
    "\n",
    "\n",
    "\n",
    "model_recs = collect_gate_metrics(vec_env, loaded_model, num_sequences=num_iters)\n",
    "\n",
    "summary_df, p_vals = compare_models_statistics(model_recs, df_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The following is test code and is inherently unstructured and messy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "gates = [\n",
    "    \"7397799322\", # 10\n",
    "    \"373373739737937373373\", #21\n",
    "    \"373373733743000773323\", #21\n",
    "    \"373323393737332975373\" #21\n",
    "    \"373373787477337373373\", #21\n",
    "    \"373373733743035223733\", #21\n",
    "    \"373373747337334323737\", #21\n",
    "    \"70747074804807470770\", #20\n",
    "    \"74715417175771747519\" #20\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_gate_composition(gate_string: str, braid_gates: list) -> np.ndarray:\n",
    "    # Initialize composition as the 5x5 identity matrix.\n",
    "    composition = np.eye(5, dtype=complex)\n",
    "\n",
    "    # Iterate over each character in the string.\n",
    "    for char in gate_string:\n",
    "        # Convert the character to an integer index.\n",
    "        try:\n",
    "            gate_index = int(char)\n",
    "        except ValueError:\n",
    "            raise ValueError(f\"Invalid character '{char}' in gate string. Must be a digit.\")\n",
    "\n",
    "        # Check that the index is within the valid range.\n",
    "        if gate_index < 0 or gate_index >= len(braid_gates):\n",
    "            raise IndexError(f\"Gate index {gate_index} is out of bounds for the available braid gates.\")\n",
    "\n",
    "        # Multiply the current composition by the braid gate.\n",
    "        composition = composition @ braid_gates[gate_index]\n",
    "    \n",
    "    return composition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gate_metrics(gate_strings, env, csv_filename=\"gate_metrics.csv\"):\n",
    "    # Determine the maximum width for the \"Operator\" column for nice formatting.\n",
    "    max_operator_width = max(len(str(g)) for g in gate_strings)\n",
    "    operator_column_width = max(max_operator_width, 10)  # Ensure at least 10 characters for aesthetics\n",
    "\n",
    "    # Print table header to the console\n",
    "    header = (f\"{'Operator':<{operator_column_width}} | \"\n",
    "              f\"{'Closeness':<10} | {'Leakage':<10} | {'Unitarity':<10}\")\n",
    "    print(header)\n",
    "    print(\"-\" * (operator_column_width + 34))\n",
    "\n",
    "    # Open CSV file for writing the results.\n",
    "    with open(csv_filename, \"w\", newline=\"\") as csvfile:\n",
    "        fieldnames = [\"Operator\", \"Closeness\", \"Leakage\", \"Unitarity\"]\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        # Process each gate string.\n",
    "        for gate_str in gate_strings:\n",
    "            # Reset the current composition in the environment.\n",
    "            env.reset_composition()\n",
    "            \n",
    "            # Compute the gate composition for the given string.\n",
    "            gate_matrix = env.get_gate_composition(gate_str)\n",
    "            \n",
    "            # Update the environment's composition to the computed gate.\n",
    "            env.current_composition = gate_matrix\n",
    "            \n",
    "            # Compute the metrics for the current composition.\n",
    "            leakage, closeness_error, unitarity_error, total_error = env.compute_reward()\n",
    "            \n",
    "            # Optionally, set unitarity error to exactly zero if very small.\n",
    "            if unitarity_error < 0.0001:\n",
    "                unitarity_error = 0.0\n",
    "            \n",
    "            # Format and print the row with the gate metrics.\n",
    "            print(f\"{gate_str:<{operator_column_width}} | \"\n",
    "                  f\"{closeness_error:<10.3e} | \"\n",
    "                  f\"{leakage:<10.3f} | \"\n",
    "                  f\"{unitarity_error:<10.3e}\")\n",
    "            \n",
    "            # Write the row to the CSV file\n",
    "            writer.writerow({\n",
    "                \"Operator\": gate_str,\n",
    "                \"Closeness\": f\"{closeness_error:.3e}\",\n",
    "                \"Leakage\": f\"{leakage:.3f}\",\n",
    "                \"Unitarity\": f\"{unitarity_error:.3e}\"\n",
    "            })\n",
    "\n",
    "    print(f\"Gate metrics have been written to {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "test = [\"30\",\n",
    "        \"880\",\n",
    "        \"5300\",\n",
    "        \"48440\",\n",
    "        \"553000\",\n",
    "        \"4334300\",\n",
    "        \"37770000\",\n",
    "        \"441001048\",\n",
    "        \"2555337582\",\n",
    "        \"59672955693\",\n",
    "        \"374050007970\",\n",
    "        \"306947382105\",\n",
    "        \"6595888969003\",\n",
    "        \"75139699757375\",\n",
    "        \"78450807782427077787\",\n",
    "        \"287430798424700981936977\"\n",
    "        ]\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "test = [\n",
    "    \"000\",\n",
    "    \"0000\",\n",
    "    \"00000\",\n",
    "    \"222000\",\n",
    "    \"0000000\",\n",
    "    \"00000000\",\n",
    "    \"000000000\",\n",
    "    \"2221001222\",\n",
    "    \"22210012220\",\n",
    "    \"223443100122\",\n",
    "    \"2213404301242\",\n",
    "    \"34224334310031224\",\n",
    "    \"242314040312211412221\",\n",
    "    \"422211222020214000112021\",\n",
    "    \"101422102223130431111322222\",\n",
    "    \"31423322434332420442310301422\",\n",
    "    \"222223043422422024043333320003\",\n",
    "    \"2344130440331032213334400344312\",\n",
    "    \"423330314001220244224333334034032\",\n",
    "    \"132424244140142042111112011202122020400\",\n",
    "    \"231333001244422220433403402422343302043\"\n",
    "]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_gates = [\n",
    "    \"373373739737937373373\", #21\n",
    "    \"373373733743000773323\", #21\n",
    "    \"373323393737332975373\" #21\n",
    "    \"373373787477337373373\", #21\n",
    "    \"373373733743035223733\", #21\n",
    "    \"373373747337334323737\", #21\n",
    "    \"70747074804807470770\", #20\n",
    "    \"74715417175771747519\", #20\n",
    "    \"124543537528586425295552041721248\", \n",
    "    \"3304508631218099892215222273419788357195\",\n",
    "    \"2522122520252241121222952\",\n",
    "    \"2122521290220262257522525\",\n",
    "    \"212212922221206520262657\",\n",
    "    \"252282524225252528922929527272252\",\n",
    "    \"29220239322922922922920222292927209229\",\n",
    "    \"7545960196017659606465541914747\",\n",
    "    \"226262460644262626167206626272090226\",\n",
    "    \"246060662626072722621262727272727206626\",\n",
    "    \"272776262720226242026442267\",\n",
    "    \"27272722626202724625625226244616\",\n",
    "    \"27226262721242606620229262026226262622\",\n",
    "    \"7397799322\",\n",
    "    \"373373739737937373373\",\n",
    "    \"373373733743000773323\",\n",
    "    \"2262620207742620262\",\n",
    "    \"206724676460242606626764692627272720202\",\n",
    "    \"226262020662676402622629720127202662\",\n",
    "    \"2262627262020662620962262\",\n",
    "    \"22626246064426262606626242026\",\n",
    "    \"2262624672064427262672722672066262427242\",\n",
    "    \"272262627246727292020662620962267922\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_sequence(seq: str) -> str:\n",
    "    \n",
    "    # Inverse mapping: each key is the inverse of its value.\n",
    "    inv_map = {\n",
    "        '0': '5', '1': '6', '2': '7', '3': '8', '4': '9',\n",
    "        '5': '0', '6': '1', '7': '2', '8': '3', '9': '4'\n",
    "    }\n",
    "    \n",
    "    stack = []\n",
    "    for char in seq:\n",
    "        # Check if the last element in stack is the inverse of current char.\n",
    "        if stack and inv_map[char] == stack[-1]:\n",
    "            stack.pop()\n",
    "        else:\n",
    "            stack.append(char)\n",
    "    \n",
    "    return ''.join(stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_found_gates = []\n",
    "\n",
    " # Process and print the reduced sequences\n",
    "for i, seq in enumerate(found_gates, 1):\n",
    "    reduced = reduce_sequence(seq)\n",
    "    print(f\"Sequence {i}: Original: {seq}\")\n",
    "    print(f\"            Reduced:  {reduced}\\n\")\n",
    "    reduced_found_gates.append(reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GateApproxEnv(\n",
    "    braid_gates=braid_gates,\n",
    "    target_gate=target_gate,\n",
    "    subs=subs,\n",
    "    max_length=SEQUENCE_LENGTH,\n",
    "    alpha=ALPHA,\n",
    "    beta=BETA,\n",
    "    gamma=GAMMA,\n",
    "    local_equivalence_class=True\n",
    ")\n",
    "print_gate_metrics(reduced_found_gates, env)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
